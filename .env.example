PORT=8000
HOST=0.0.0.0

# =============================================================================
# Provider Configuration
# =============================================================================
# Each provider can be configured globally via environment variables.
# Additionally, you can override per-model settings in model-config.yaml
# using provider_config with environment variable references (e.g., "$API_KEY").

# Generic OpenAI provider
# Configure if you have a direct OpenAI-compatible API endpoint
OPENAI_BASE_URL=https://api.openai.com
OPENAI_API_KEY=sk-your-openai-key

# OpenRouter provider
# Configure if using OpenRouter with provider selection capabilities
OPENROUTER_API_KEY=sk-or-your-openrouter-key
# Optional: specify preferred providers (comma-separated)
# Only set if you want to restrict to specific providers
# OPENROUTER_PROVIDERS=OpenAI,Anthropic
# Optional: selection order (array of provider slugs)
# OPENROUTER_ORDER=anthropic,openai
# Optional: sort strategy - "price", "throughput", or "latency"
# OPENROUTER_SORT=price
# Optional: allow fallbacks (default: true)
# OPENROUTER_ALLOW_FALLBACKS=true
# Optional: base URL (defaults to https://openrouter.ai/api if not set)
# Only override if using a different OpenRouter endpoint
# OPENROUTER_BASE_URL=https://openrouter.ai/api
# Optional: model shortcut (e.g., ":nitro" for throughput, ":floor" for price)
# OPENROUTER_MODEL_SHORTCUT=:nitro

# Vertex AI MaaS provider (optional)
# GOOGLE_APPLICATION_CREDENTIALS can be a full JSON string or the absolute path to the service-account file.
VERTEX_PROJECT_ID=your-project
VERTEX_LOCATION=us-central1
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
# Optional custom endpoint for Vertex OpenAI-compatible API
# VERTEX_CHAT_ENDPOINT=https://example.com/v1/projects/.../endpoints/openapi/chat/completions

# =============================================================================
# Routing Configuration
# =============================================================================
# Model config can be inline YAML or a path to a YAML file.
# The configuration supports per-model provider overrides via provider_config.
# See examples in model-config.example.yaml

# Inline model config (alternative to file)
# MODEL_CONFIG="default_strategy: round_robin\nmodels:\n  - name: gpt-4o\n    provider: openai\n    model: openai/gpt-4o"

# Path to model config file
MODEL_CONFIG_PATH=./model-config.yaml

# =============================================================================
# Logging Configuration
# =============================================================================
# SQLite log database path
LOG_DB_PATH=./data/logs.db

# Optional: SSE token streaming artifical delay (ms) and chunk size
# Adjust for smoother UI
STREAM_DELAY=10
STREAM_CHUNK_SIZE=5

